{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2885f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:06:31.936486: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-16 14:06:32.196140: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-16 14:06:33.160775: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc634ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelfile\n",
      "Modelfile:Zone.Identifier\n",
      "json_extraction_dataset_500.json\n",
      "json_extraction_dataset_500.json:Zone.Identifier\n",
      "test.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d70e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"Extract the product information:\\n<div class='product'><h2>iPad Air</h2><span class='price'>$1344</span><span class='category'>audio</span><span class='brand'>Dell</span></div>\", 'output': {'name': 'iPad Air', 'price': '$1344', 'category': 'audio', 'manufacturer': 'Dell'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file = json.load(open(\"json_extraction_dataset_500.json\", \"r\"))\n",
    "print(file[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a8b2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# For GPU check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a099230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17542/4282502808.py:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.3: Fast Mistral patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 2048  # Choose sequence length\n",
    "dtype = None  # Auto detection\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80097163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    return f\"### Input: {example['input']}\\n### Output: {json.dumps(example['output'])}<|endoftext|>\"\n",
    "\n",
    "formatted_data = [format_prompt(item) for item in file]\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6819058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # LoRA rank - higher = more capacity, more memory\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Rank stabilized LoRA\n",
    "    loftq_config=None, # LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62537a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=28): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 692.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments optimized for Unsloth\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=25,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa76de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 3 | Total steps = 189\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 119,537,664 of 3,940,617,216 (3.03% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 02:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a57b533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> Extract the product information:\n",
      "<div class='product'><h2>iPad Air</h2><span class='price'>$1344</span><span class='category'>audio</span><span class='brand'>Dell</span></div><|end|><|assistant|> {\"name\": \"iPad Air\", \"price\": \"$1344\", \"category\": \"audio\", \"manufacturer\": \"Dell\"}<|end|>\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Test prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Extract the product information:\\n<div class='product'><h2>iPad Air</h2><span class='price'>$1344</span><span class='category'>audio</span><span class='brand'>Dell</span></div>\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8cfdb",
   "metadata": {},
   "source": [
    "## Load from checkpoint and merge\n",
    "\n",
    "If the model above didn't have adapters, you can load from the saved checkpoint and merge it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dab849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: outputs/checkpoint-189\n",
      "==((====))==  Unsloth 2025.10.3: Fast Mistral patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚úì LoRA adapters loaded from checkpoint\n",
      "‚úì Model merged and saved to 'merged_model/' directory\n"
     ]
    }
   ],
   "source": [
    "# Load from the latest checkpoint and merge LoRA adapters\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import os\n",
    "\n",
    "# Find the latest checkpoint\n",
    "checkpoint_dir = \"outputs\"\n",
    "checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
    "if checkpoints:\n",
    "    # Sort by checkpoint number\n",
    "    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "    print(f\"Loading from checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Load base model\n",
    "    base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=False,  # Load in full precision for merging\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapters from checkpoint\n",
    "    model_with_lora = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    print(\"‚úì LoRA adapters loaded from checkpoint\")\n",
    "    \n",
    "    # Merge and save\n",
    "    merged_model = model_with_lora.merge_and_unload()\n",
    "    merged_model.save_pretrained(\"merged_model\")\n",
    "    tokenizer.save_pretrained(\"merged_model\")\n",
    "    print(\"‚úì Model merged and saved to 'merged_model/' directory\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoints found in outputs/ directory\")\n",
    "    print(\"Make sure training completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655b68a",
   "metadata": {},
   "source": [
    "## Convert to GGUF Using llama.cpp CLI (After saving merged model)\n",
    "\n",
    "After saving the merged model above, you can convert it to GGUF format using llama.cpp command-line tools. This is much more reliable than Unsloth's built-in export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeeac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then convert your merged model from bash terminal:\n",
    "./convert_to_gguf.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4705fc5",
   "metadata": {},
   "source": [
    "## Option 3: Use Ollama (Easiest for Deployment)\n",
    "\n",
    "If you want to use the model locally with Ollama, you can create a Modelfile and import it directly from the merged HuggingFace model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f369d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Steps:\n",
    "# 1. Create a file named \"Modelfile\" with this content:\n",
    "#ROM ./gguf_model/model.gguf\n",
    "#\n",
    "#PARAMETER temperature 0.7\n",
    "#PARAMETER top_p 0.9\n",
    "#PARAMETER stop \"<|endoftext|>\"\n",
    "#\n",
    "#TEMPLATE \"\"\"{{ if .System }}<|system|>\n",
    "#{{ .System }}<|end|>\n",
    "#{{ end }}{{ if .Prompt }}<|user|>\n",
    "#{{ .Prompt }}<|end|>\n",
    "#<|assistant|>\n",
    "#{{ end }}{{ .Response }}\"\"\"\n",
    "\n",
    "#SYSTEM \"\"\"You are a helpful assistant that extracts structured JSON data from HTML content.\"\"\"\n",
    "\n",
    "# 2. Run in terminal:\n",
    "#    ollama create my-finetuned-model -f Modelfile\n",
    "#\n",
    "# 3. Test it:\n",
    "#    ollama run my-finetuned-model \"Extract product info from: <div>...</div>\"\n",
    "\n",
    "print(\"Use Ollama to import the merged model - see instructions above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb88a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
